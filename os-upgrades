OS Upgrades
kubectl drain node-1
kubectl cordon node-2
kubectl uncordon node-1

kubectl drain node01 --ignore-daemonsets --force

if there is single pod with is not part of replicaset you can't drain that node
kubectl get pods -o wide --force

Run: kubectl get pods -o wide and you will see that there is a single pod scheduled on node01 which is not part of a replicaset.
The drain command will not work in this case. To forcefully drain the node we now have to use the --force flag.

if pods are not part of replicaset it will lost forever after you fire a command
kubectl drain node01 --ignore-daemonsets --force

Do not drain node01, instead use the kubectl cordon node01 command. 
This will ensure that no new pods are scheduled on this node and the existing pods will not be affected by this operation.
===================================================================
1)
This lab tests your skills on upgrading a kubernetes cluster. We have a production cluster with applications running on it. Let us explore the setup first.
how many nodes are part of cluster
number of deployment
What nodes are the pods hosted on?


kubectl get nodes
 
 
 
 2)How many nodes can host workloads in this cluster?
 By running the kubectl describe node command, we can see that neither nodes have taints.

root@controlplane:~# kubectl describe nodes  controlplane | grep -i taint
Taints:             <none>
root@controlplane:~# 
root@controlplane:~# kubectl describe nodes  node01 | grep -i taint
Taints:             <none>
root@controlplane:~# 

This means that both nodes have the ability to schedule workloads on them

3)
You are tasked to upgrade the cluster.
User's accessing the applications must not be impacted. And you cannot provision new VMs. What strategy would you use to upgrade the cluster?
In order to ensure minimum downtime, upgrade the cluster one node at a time, 
while moving the workloads to another node. In the upcoming tasks you will get to practice how to do that.

4)We will be upgrading the master node first. Drain the master node of workloads and mark it UnSchedulable
Master Node: SchedulingDisabled
a)kubectl drain controlplane --ignore-daemonsets

5)Upgrade the controlplane components to exact version v1.20.0

Upgrade kubeadm tool (if not already), then the master components, and finally the kubelet.
Practice referring to the kubernetes documentation page. Note: While upgrading kubelet,
if you hit dependency issue while running the apt-get upgrade kubelet command, use the apt install kubelet=1.20.0-00 command instead


On the controlplane node, run the command run the following commands:

    apt update
    This will update the package lists from the software repository.

    apt install kubeadm=1.20.0-00
    This will install the kubeadm version 1.20

    kubeadm upgrade apply v1.20.0
    This will upgrade kubernetes controlplane. Note that this can take a few minutes.

    apt install kubelet=1.20.0-00 This will update the kubelet with the version 1.20.

    You may need to restart kubelet after it has been upgraded.
    Run: systemctl restart kubelet
    
    
    =====
    
    6)Mark the controlplane node as "Schedulable" again
    kubectl uncordon controlplane
    
    7)Next is the worker node. Drain the worker node of the workloads and mark it UnSchedulable
    kubectl drain node01 --ignore-daemonsets
    
    kubectl get pods -o wide
    
    
    7)Upgrade the worker node to the exact version v1.20.0
    
    On the node01 node, run the command run the following commands:

    If you are on the master node, run ssh node01 to go to node01

    apt update
    This will update the package lists from the software repository.

    apt install kubeadm=1.20.0-00
    This will install the kubeadm version 1.20

    kubeadm upgrade node
    This will upgrade the node01 configuration.

    apt install kubelet=1.20.0-00 This will update the kubelet with the version 1.20.

    You may need to restart kubelet after it has been upgraded.
    Run: systemctl restart kubelet

    Type exit or enter CTL + d to go back to the controlplane node.
    
    =====================================================================================
    
    Backup and restore
    =====================
    
 Look at the ETCD Logs using the command kubectl logs etcd-controlplane -n kube-system or 
 check the image used by the ETCD pod: kubectl describe pod etcd-controlplane -n kube-system

root@controlplane:~# kubectl -n kube-system logs etcd-controlplane | grep -i 'etcd-version'
"ts":"2022-03-25T08:45:18.872Z","caller":"embed/etcd.go:307","msg":"starting an etcd server","etcd-version":"3.5.1","git-sha":"e8732fb5f"
root@controlplane:~# 
root@controlplane:~# kubectl -n kube-system describe pod etcd-controlplane | grep Image:
    Image:         k8s.gcr.io/etcd:3.5.1-0
root@controlplane:~#

===

At what address can you reach the ETCD cluster from the controlplane node?

Check the ETCD Service configuration in the ETCD POD
    
root@controlplane:~# kubectl -n kube-system describe pod etcd-controlplane | grep '\--listen-client-urls'
      --listen-client-urls=https://127.0.0.1:2379,https://10.2.43.11:2379

This means that ETCD is reachable on localhost (127.0.0.1) at port 2379.


==========

Where is the ETCD server certificate file located?

Note this path down as you will need to use it later

Check the ETCD pod configuration with the command: kubectl describe pod etcd-controlplane -n kube-system and look for the value for --cert-file:

root@controlplane:~# kubectl -n kube-system describe pod etcd-controlplane | grep '\--cert-file'
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
root@controlplane:~#


===========
Where is the ETCD CA Certificate file located?

Note this path down as you will need to use it later.

Check the ETCD pod configuration with the command: kubectl describe pod etcd-controlplane -n kube-system and look for the value of --trusted-ca-file:

root@controlplane:~# kubectl -n kube-system describe pod etcd-controlplane | grep '\--trusted-ca-file'
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
root@controlplane:~#


==========

The master node in our cluster is planned for a regular maintenance reboot tonight.
While we do not anticipate anything to go wrong, we are required to take the necessary backups. 
Take a snapshot of the ETCD database using the built-in snapshot functionality.

root@controlplane:~# ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /opt/snapshot-pre-boot.db

Snapshot saved at /opt/snapshot-pre-boot.db
root@controlplane:~# 

===================

Wake up! We have a conference call! After the reboot the master nodes came back online,
but none of our applications are accessible. Check the status of the applications on the cluster. What's wrong?

Are you able to see any deployments/pods or services in the default namespace?

========

Luckily we took a backup. Restore the original state of the cluster using the backup file.

First Restore the snapshot:

First Restore the snapshot:

root@controlplane:~# ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db


2022-03-25 09:19:27.175043 I | mvcc: restore compact to 2552
2022-03-25 09:19:27.266709 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
root@controlplane:~# 

Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the --data-dir.

Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory (/var/lib/etcd-from-backup).

  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data

With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want)

When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.

    Note 1: As the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can run a watch "docker ps | grep etcd" command to see when the ETCD pod is restarted.

    Note 2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane and wait 1 minute.

    Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.

If you do change --data-dir to /var/lib/etcd-from-backup in the YAML file, make sure that the volumeMounts for etcd-data is updated as well, with the mountPath pointing to /var/lib/etcd-from-backup (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)
